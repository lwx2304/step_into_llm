{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f9884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-19 12:28:47,192 - mindformers - INFO - Config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-08-19 12:28:47,232 - mindformers - INFO - Load the tokenizer name ChatGLM2Tokenizer from the ./checkpoint_download/glm2/glm2_6b.yaml\n",
      "2023-08-19 12:28:47,255 - mindformers - INFO - config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-08-19 12:28:47,277 - mindformers - WARNING - Can't find the tokenizer_config.json in the file_dict. The content of file_dict is : {}\n",
      "2023-08-19 12:28:47,287 - mindformers - INFO - build tokenizer class name is: ChatGLM2Tokenizer using args {'bos_token': '<sop>', 'eos_token': '<eop>', 'end_token': '</s>', 'mask_token': '[MASK]', 'gmask_token': '[gMASK]', 'pad_token': '<pad>', 'unk_token': '<unk>', 'vocab_file': './checkpoint_download/glm2/tokenizer.model'}.\n",
      "2023-08-19 12:28:47,367 - mindformers - INFO - ChatGLM2Tokenizer Tokenizer built successfully!\n",
      "2023-08-19 12:30:55,807 - mindformers - INFO - start to read the ckpt file: 12487185277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-19 12:31:41,733 - mindformers - INFO - weights in ./checkpoint_download/glm2/glm2_6b.ckpt are loaded\n",
      "2023-08-19 12:31:41,735 - mindformers - INFO - model built successfully!\n",
      "[Round 1]\n",
      "\n",
      "问：你好\n",
      "\n",
      "答： 你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\n"
     ]
    }
   ],
   "source": [
    "from mindformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"glm2_6b\")\n",
    "model = AutoModel.from_pretrained(\"glm2_6b\")\n",
    "\n",
    "query = \"你好\"\n",
    "\n",
    "prompted_inputs = tokenizer.build_prompt(query)\n",
    "input_tokens = tokenizer([prompted_inputs])\n",
    "\n",
    "outputs = model.generate(input_tokens[\"input_ids\"], max_length=100)\n",
    "response = tokenizer.decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea0b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-24 10:37:14,086 - mindformers - INFO - Config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-08-24 10:37:14,107 - mindformers - INFO - Load the tokenizer name ChatGLM2Tokenizer from the ./checkpoint_download/glm2/glm2_6b.yaml\n",
      "2023-08-24 10:37:14,126 - mindformers - INFO - config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-08-24 10:37:14,144 - mindformers - WARNING - Can't find the tokenizer_config.json in the file_dict. The content of file_dict is : {}\n",
      "2023-08-24 10:37:14,145 - mindformers - INFO - build tokenizer class name is: ChatGLM2Tokenizer using args {'bos_token': '<sop>', 'eos_token': '<eop>', 'end_token': '</s>', 'mask_token': '[MASK]', 'gmask_token': '[gMASK]', 'pad_token': '<pad>', 'unk_token': '<unk>', 'vocab_file': './checkpoint_download/glm2/tokenizer.model'}.\n",
      "2023-08-24 10:37:14,175 - mindformers - INFO - ChatGLM2Tokenizer Tokenizer built successfully!\n",
      "2023-08-24 10:39:28,886 - mindformers - INFO - start to read the ckpt file: 12487185277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-24 10:40:08,614 - mindformers - INFO - weights in ./checkpoint_download/glm2/glm2_6b.ckpt are loaded\n",
      "2023-08-24 10:40:08,621 - mindformers - INFO - model built successfully!\n",
      "欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 你好\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n",
      "\n",
      "用户：你好\n",
      "\n",
      "ChatGLM2-6B：你好，我是 ChatGLM2-6B， 一个人工智能助手。我背后使用的模型是 GLM2-6B， 是一种大型语言模型， 具有超过 2000 亿参数，支持多种任务。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： clear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 介绍华为手机\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n",
      "\n",
      "用户：介绍华为手机\n",
      "\n",
      "ChatGLM2-6B：介绍华为手机产品线，华为手机产品线包括哪些型号？ \n",
      "\n",
      "华为手机产品线包括以下型号:\n",
      "\n",
      "1. 华为P系列:P10、P10 Plus、P40、P40 Pro、P40 Pro Lite\n",
      "\n",
      "2. 华为Mate系列:Mate 10、Mate 10 Pro、Mate 20、Mate 20 Pro、Mate 30、Mate 3\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： stop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import signal\n",
    "import readline\n",
    "\n",
    "import time\n",
    "import mindspore as ms\n",
    "import numpy as np\n",
    "from mindformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"glm2_6b\")\n",
    "model = AutoModel.from_pretrained(\"glm2_6b\")\n",
    "\n",
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name == 'Windows' else 'clear'\n",
    "stop_stream = False\n",
    "\n",
    "\n",
    "def build_prompt(history):\n",
    "    prompt = \"欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\"\n",
    "    for query, response in history:\n",
    "        prompt += f\"\\n\\n用户：{query}\"\n",
    "        prompt += f\"\\n\\nChatGLM2-6B：{response}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def signal_handler(signal, frame):\n",
    "    global stop_stream\n",
    "    stop_stream = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    history = []\n",
    "    global stop_stream\n",
    "    print(\"欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\")\n",
    "    while True:\n",
    "        query = input(\"\\n用户：\")\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            history = []\n",
    "            os.system(clear_command)\n",
    "            print(\"欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\")\n",
    "            continue\n",
    "        count = 0\n",
    "\n",
    "        inputs = tokenizer(query)\n",
    "        outputs = model.generate(np.expand_dims(np.array(inputs['input_ids']).astype(np.int32), 0),\n",
    "                                 max_length=100, do_sample=False, top_p=0.7, top_k=1)\n",
    "        response = tokenizer.decode(outputs)[0]\n",
    "        history = history + [(query, response)]\n",
    "\n",
    "        if stop_stream:\n",
    "            stop_stream = False\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "            if count % 8 == 0:\n",
    "                os.system(clear_command)\n",
    "                print(build_prompt(history), flush=True)\n",
    "                signal.signal(signal.SIGINT, signal_handler)\n",
    "        os.system(clear_command)\n",
    "        print(build_prompt(history), flush=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6523c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
